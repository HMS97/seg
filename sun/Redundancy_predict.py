#!/usr/bin/env python
# coding: utf-8
from PIL import Image
import cv2
from path import Path
from utils.datasets import SlippyMapTilesConcatenation
import torch
import torch.backends.cudnn
from torch.utils.data import DataLoader
from utils.transforms import (
    JointCompose,
    JointTransform,
    JointRandomHorizontalFlip,
    JointRandomRotation,
    ConvertImageMode,
    ImageToTensor,
    MaskToTensor,
)
from torchvision.transforms import Resize, CenterCrop, Normalize
import os
import tqdm
import numpy as np
import argparse
import shutil
import glob
import tifffile as tiff
from PIL import Image
from skimage import io
import matplotlib.image as mpimg
# import libtiff

# need to create a file to store temp pictures
if not os.path.exists('temp_pic'):
    os.makedirs('temp_pic')
path = './temp_pic/'
device = 'cuda'


## use model to predict
def predict(model):
    model.eval()
    result = []
    for images in tqdm.tqdm(test_loader):
        images = images.to(device,dtype=torch.float)
        outputs = model(images)
        probs = torch.max(outputs,1)[1]
        result.append(probs.cpu().numpy())
    return result


def input_and_output(pic_path, model, generate_data):
    """
    args:
        pic_path : the picture you want to predict
        model    : the model you want to predict
    note:
        step one : generate some pictures from one picture
        step two : predict from the images generated by step one 
    """
    image_size = args.crop_size

    image = io.imread(pic_path)
    h,w,_ = image.shape

    padding_h = (h // image_size + 1) * image_size
    padding_w = (w // image_size + 1) * image_size
    row = padding_h//image_size
    col = padding_w//image_size
    padding_img = np.zeros((padding_h, padding_w, 3), dtype=np.uint8)
    padding_img[0:h, 0:w, :] = image[:, :, 1:]

    padding_img = np.array(padding_img)
#     print ('src:',padding_img.shape)
    mask_whole = np.zeros((row*image_size, col*image_size), dtype=np.uint8)
    if generate_data == False:
        result = predict(model)
        map_list = [str(i.name) for i in Path('temp_pic').files()]
    for i in range(row):
        for j in range(col):
            if generate_data:
                crop = redundancy_crop(padding_img, i, j, row, col, image_size)
                ch,cw,_ = crop.shape
                cv2.imwrite(f'temp_pic/{i}_{j}.png',crop)
            else:
                temp = result[map_list.index(f'{i}_{j}.png')]
                temp = redundancy_crop2(temp, i, j, row, col)
                mask_whole[i*image_size:i*image_size+image_size,j*image_size:j*image_size+image_size] = temp
    return mask_whole[0:h, 0:w]


def redundancy_crop(img, i, j, row, col, targetSize):
    if (i+1) ==1 or (i+1) == row or (j+1)==1 or (j+1)==col:
        temp_img = img[i*targetSize:i*targetSize+targetSize, j*targetSize:j*targetSize+targetSize, :]
    else:
        temp_img = img[i*targetSize-args.padding_size:i*targetSize+targetSize+args.padding_size, j*targetSize-args.padding_size:j*targetSize+targetSize+args.padding_size, :]
    return temp_img


def redundancy_crop2(img, i, j, row, col):
    if (i + 1) == 1 or (i + 1) == row or (j + 1) == 1 or (j + 1) == col:
        return img
    else:
        h = img.shape[1]
        w = img.shape[2]
        temp_img = img[:,args.padding_size:h-args.padding_size,args.padding_size:w-args.padding_size]
    return temp_img


def get_dataset_loaders( workers):
    target_size = 512
    batch_size = 1

    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]

    transform = JointCompose(
        [
            JointTransform(Resize(target_size, Image.BILINEAR), Resize(target_size, Image.NEAREST)),
            JointTransform(CenterCrop(target_size), CenterCrop(target_size)),
            JointRandomHorizontalFlip(0.5),
            JointRandomRotation(0.5, 90),
            JointRandomRotation(0.5, 90),
            JointRandomRotation(0.5, 90),
            JointTransform(ImageToTensor(), MaskToTensor()),
            JointTransform(Normalize(mean=mean, std=std), None),
        ]
    )

    test_dataset = SlippyMapTilesConcatenation(
        os.path.join(path),'./' , transform,debug = False,test  = True
    )

    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=workers)

    return test_loader


def get_labels():
    """Load the mapping that associates classes with label colors

    Returns:
        np.ndarray with dimensions (13, 3)
    """
    return np.array([
        [  0,   0,   0],
        [0, 200 ,0],
       [150, 250,   0],
       [150, 200, 150],
       [200,   0, 200],
       [150,   0, 250],
       [150, 150, 250],
       [250, 200,   0],
       [200, 200,   0],
       [200,   0,   0],
       [250,   0, 150],
       [200, 150, 150],
       [250, 150, 150],
       [  0,   0, 200],
       [  0, 150, 200],
       [  0, 200, 250]])


def decode_segmap(label_mask, n_classes):
    """Decode segmentation class labels into a color image

    Args:
        label_mask (np.ndarray): an (M,N) array of integer values denoting
          the class label at each spatial location.
        plot (bool, optional): whether to show the resulting color image
          in a figure.

    Returns:
        (np.ndarray, optional): the resulting decoded color image.
    """
    label_colours = get_labels()
    r = label_mask.copy()
    g = label_mask.copy()
    b = label_mask.copy()
    for ll in range(0, n_classes):
        r[label_mask == ll] = label_colours[ll, 0]
        g[label_mask == ll] = label_colours[ll, 1]
        b[label_mask == ll] = label_colours[ll, 2]
    rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))
    rgb[:, :, 0] = r
    rgb[:, :, 1] = g
    rgb[:, :, 2] = b
    return rgb.astype(np.uint8)


if __name__ =="__main__":
    parse = argparse.ArgumentParser()
    parse.add_argument("--n_class", type=int, default=16, help="the number of classes")
    parse.add_argument("--model_name", type=str, default='UNet', help="UNet,pspnet,FPN")

    parse.add_argument("--n_workers", type=int, default=4, help="the number of workers")
    parse.add_argument("--crop_size", type=int, default=512, help="the number of workers")
    parse.add_argument("--padding_size", type=int, default=32, help="the number of workers")

    args = parse.parse_args()

    # predict on one model
    model = torch.load("./model/unet_2019-07-24_swa.pth")

    imgList = glob.glob("./test/*(2).tif")
    num = len(imgList)

    save_path = f'./result'
    if not os.path.exists(save_path):
        os.makedirs(save_path)

    for i in tqdm.tqdm(range(num)):
        if not os.path.exists('temp_pic'):
            os.makedirs('temp_pic')
        ### predict on one picture
        input_and_output(imgList[i], model, generate_data=True)
        name = os.path.split(imgList[i])[-1].split(".")[0][:-4]
        test_loader = get_dataset_loaders(args.n_workers)
        mask_result = input_and_output(imgList[i], model, generate_data=False)
        numlist = np.unique(np.array(mask_result))
        print(numlist)
        # 递归删除文件夹
        try:
            shutil.rmtree('temp_pic')
        except:
            pass
        decoded = decode_segmap(mask_result, args.n_class)[:, :, ::-1]
        # print(mask_result.shape)
        cv2.imwrite(f'{save_path}/{name}_label.tif', decoded)